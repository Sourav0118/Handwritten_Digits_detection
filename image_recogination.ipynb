{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6da1e6",
   "metadata": {},
   "source": [
    "# Image Recogination using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054a309",
   "metadata": {},
   "source": [
    "## Exporting the data\n",
    "#### we begin by importing torch and torchvision.  torchvision contains some utilities for working with image data. it alos contains the helper classes to automatically dowmload and import popular dataset like MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224faa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as trn\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fd21cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'D:\\\\sourav\\\\cs\\\\AI_ML\\\\pytorch\\\\pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msourav\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAI_ML\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/souravsaini/Data/POP_OS/env/lib/python3.10/site-packages/torchvision/datasets/mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/souravsaini/Data/POP_OS/env/lib/python3.10/site-packages/torchvision/datasets/mnist.py:179\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# download files\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources:\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'D:\\\\sourav\\\\cs\\\\AI_ML\\\\pytorch\\\\pytorch'"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root = \"D:\\sourav\\cs\\AI_ML\\pytorch\\pytorch\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca32ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07553159",
   "metadata": {},
   "source": [
    "the dataset has 60,000 images which can be used to train the model. there is also an aditional test set of 10,000 images which can be created by passing train=False to the MNIST class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root = \"D:\\sourav\\cs\\AI_ML\\pytorch\\pytorch\", train = False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abc9c8",
   "metadata": {},
   "source": [
    "it's a pair consiting of a 28x28 image and a lable. the image is an object of the class PIL.Image.Image, which is a part of the python imaging library Piloow. we can view the image within jupyter matlpotlib, the de-facto lpotting and graphing library for the data science in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81270997",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')  #cmap = \"gray\" tells that image is combinations of shades of grey\n",
    "print(\"label: \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b342a66",
   "metadata": {},
   "source": [
    "## Convert images to pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd4d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root = \"D:\\sourav\\cs\\AI_ML\\pytorch\\pytorch\", train=True, transform = trn.ToTensor())\n",
    "img_t, label = dataset[0]\n",
    "print(img_t.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_t[0:, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t[0, 10:15, 10:15], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4b080",
   "metadata": {},
   "source": [
    "### Traning set-\n",
    "##### used to train the model i.e compute the loss and adjust the weights of the model using gradient descent\n",
    "### Validation set-\n",
    "\n",
    "##### used to evaluate the model while training, adjust hyperparameter (learning rate, e.t.c.) and pick the best version of the model\n",
    "\n",
    "### Test set-\n",
    "##### used to compute different models, or different types of modleing approaches and report the finaccuracy of the model\n",
    "\n",
    "\n",
    "#### there are no predefined validation set, therefore we must manually split the 60,000 images into traing and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define a function that randomly picks a given fraction of the images for the validation set\n",
    "\n",
    "np.set_printoptions(suppress = True)\n",
    "def split_indx(n, val_frct):\n",
    "    n_val = int(n*val_frct)   # no. of images x faction\n",
    "    idx = np.random.permutation(n)   # creating random permutation of 0 to n-1\n",
    "    return idx[n_val:], idx[:n_val]\n",
    "\n",
    "train_idx, val_idx = split_indx(len(dataset), 0.15)\n",
    "print(len(train_idx))\n",
    "print(len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "print(train_sampler)\n",
    "train_loader = DataLoader(dataset, batch_size, sampler=train_sampler)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "val_loader = DataLoader(dataset, batch_size, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bebb9",
   "metadata": {},
   "source": [
    "since nn.linear expects the each training examples to be a vactor, each 1x28x28 image tensor needs to be flattened out into a vector of size(28x28)\n",
    "\n",
    "the output of ecah image is the vector  of size 10, with each element of the vector signifying the probability of a particular target label(i.e. 0 to 9). the predicted label for an image is simply the one with teh highest probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress = True)\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120543c9",
   "metadata": {},
   "source": [
    "since our images are of the shape 1x28x28 but we need them to be a vector of size 784 i.e. we need to flatten them out. we will use .reshape method of tensor which will allow us to efficiently view each image as a flat vector. \n",
    "to include this additional functionality within our model, we need to define a custom model by extending the nn.Module class from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # to inherit the nn.Module class\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        # print('self.linear= ', self.linear)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # print('old xb size= ', xb.shape)\n",
    "        # print('old xb= ', xb)\n",
    "        xb = xb.reshape(-1,784)  # -1 allows to work with any batch size\n",
    "        # print('xb size= ', xb.shape)\n",
    "        # print('xb= ', xb)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "\n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    otpt = model(images)\n",
    "    print('otpt size=', otpt.shape)\n",
    "    print(otpt)\n",
    "    break\n",
    "print(model.linear.weight.shape)\n",
    "print(model.linear.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25359eb9",
   "metadata": {},
   "source": [
    "now we want to convert the each data value in the weigth as probabilities and therefore the values should be in range[0,1] and sum of all the probabilties should add to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a9700",
   "metadata": {},
   "source": [
    "to do so we first raise all the values(vi) in the row to e^(vi). this is to make a significant difference between the numbers. then we divide the value by sum of all the elements. the value we got is called the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2172b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = F.softmax(otpt, dim=1)\n",
    "print(prob)\n",
    "print(prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9b0a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1418, 0.1459, 0.1252, 0.1173, 0.1334], grad_fn=<MaxBackward0>)\n",
      "torch.Size([5])\n",
      "tensor([2, 2, 6, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "max_prob, pred = torch.max(prob, dim=1)\n",
    "print(max_prob)\n",
    "print(max_prob.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc29cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734d84a",
   "metadata": {},
   "source": [
    "# Evaluation matrix and Loss function\n",
    "#### just as with linear regression, we need a way to evaluate how well our model is performing. A natural way to do is to find the percentage of lables that were predicted correctly i.e. the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f483c4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(l1, l2):\n",
    "    return torch.sum(l1==l2).item()/len(l1)\n",
    "\n",
    "accuracy(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c347f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False, False])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels==pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2b4e8",
   "metadata": {},
   "source": [
    "##### we can not use accuracy as our loss function, because accuray is non continous and differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c411a4",
   "metadata": {},
   "source": [
    "##### a commonly used loss fnx for clarrification problem is the cross entropy which can be done in the following way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9a54e",
   "metadata": {},
   "source": [
    "##### for each output row pick the predicted probability for the correct label.\n",
    "\n",
    "##### then take the log of the picked value. if the probability is high or close to 1 then the log value will be very small negative value close to zero. if the probabilty is close to one then the log value will be very high negative value. so we multiply the value with -1 which makes it positive\n",
    "\n",
    "##### then take the avg value of the entory across all the output row to get the overall loss for the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a79db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241m.\u001b[39mcross_entropy\n\u001b[1;32m      2\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(prob, labels)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "loss = loss_fn(prob, labels)\n",
    "print(loss)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a89aa",
   "metadata": {},
   "source": [
    "# Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc433873",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = 0.001\n",
    "opt = torch.optim.SGD(model.parameters(), lr=l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfc61a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_fxn, xb, yb, opt=None, metric=None):\n",
    "    preds = model(xb)   # find the predictions by putting the batch into the model fxn\n",
    "    loss = loss_fxn(preds, yb) # then find the loss wrt to the lables provided\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()   # compute gradient\n",
    "        opt.step()        # update parameters\n",
    "        opt.zero_grad()   # reset gradients\n",
    "    \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(preds, yb)\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588c7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loss_fn, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "        # for gives the batches out of the validation set\n",
    "        # metric is the accuracy matrix\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric=metric)\n",
    "                  for xb, yb in valid_dl]\n",
    "        # to separate out the lists we use zip function\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        # total size is the sum of all the batch size\n",
    "        total = np.sum(nums)\n",
    "        # since we have divded the set into batches therefore we can have the last batch size to be smaller than the rest\n",
    "        # so we first calculate the length and then find its avg value\n",
    "        # suppose we have a batch of size 3, 3 and 2. so we take the first loss and multipy with 3, take 2nd and multiply with \n",
    "        # 3 and then take 3rd loss and muliply with 2. ten sum all the values and then divide by sum of batch size i.e. 3+3+2\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums))/total\n",
    "        return avg_loss, total, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09efbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, lables):\n",
    "    prob, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.sum(pred == lables).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643d4f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20368\\1821637592.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss= \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" accuracy= \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluation(model, loss_fn, val_loader, accuracy)\n",
    "print(\"loss= \", val_loss, \" accuracy= \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b73968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    for i in range(epochs):\n",
    "        # Training\n",
    "        for xb, yb in train_dl:\n",
    "            loss, length, metric_result = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "        \n",
    "        # Evaluation\n",
    "        result = evaluation(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        # printing the progress\n",
    "        if metric is None:\n",
    "            print(\"epoch [\", i+1, ',', epochs, '] , loss= ', val_loss)\n",
    "        else:\n",
    "            print('*Epoch [', i+1, ',', epochs, '], loss= ', val_loss, ',', metric.__name__, ',' , val_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52074732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr= l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5be3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Epoch [ 1 , 30 ], loss=  0.6083059507980942 , accuracy , 0.09988888888888889\n",
      "*Epoch [ 2 , 30 ], loss=  0.4897251421585679 , accuracy , 0.09333333333333334\n",
      "*Epoch [ 3 , 30 ], loss=  0.4412181560208814 , accuracy , 0.0971111111111111\n",
      "*Epoch [ 4 , 30 ], loss=  0.41414751435733504 , accuracy , 0.09722222222222222\n",
      "*Epoch [ 5 , 30 ], loss=  0.39567119393523575 , accuracy , 0.10177777777777777\n",
      "*Epoch [ 6 , 30 ], loss=  0.3821566343302321 , accuracy , 0.097\n",
      "*Epoch [ 7 , 30 ], loss=  0.37222685020572194 , accuracy , 0.10133333333333333\n",
      "*Epoch [ 8 , 30 ], loss=  0.3634978203873874 , accuracy , 0.09677777777777778\n",
      "*Epoch [ 9 , 30 ], loss=  0.35746556281215613 , accuracy , 0.095\n",
      "*Epoch [ 10 , 30 ], loss=  0.35232253602912855 , accuracy , 0.10211111111111111\n",
      "*Epoch [ 11 , 30 ], loss=  0.3462709190581356 , accuracy , 0.09566666666666666\n",
      "*Epoch [ 12 , 30 ], loss=  0.34194930695281883 , accuracy , 0.095\n",
      "*Epoch [ 13 , 30 ], loss=  0.33812662952088024 , accuracy , 0.09822222222222222\n",
      "*Epoch [ 14 , 30 ], loss=  0.33506298046378 , accuracy , 0.10355555555555555\n",
      "*Epoch [ 15 , 30 ], loss=  0.3320606010505516 , accuracy , 0.09811111111111111\n",
      "*Epoch [ 16 , 30 ], loss=  0.32913333656835475 , accuracy , 0.09288888888888888\n",
      "*Epoch [ 17 , 30 ], loss=  0.32734722175570724 , accuracy , 0.098\n",
      "*Epoch [ 18 , 30 ], loss=  0.3254964880396922 , accuracy , 0.09666666666666666\n",
      "*Epoch [ 19 , 30 ], loss=  0.32365883118059074 , accuracy , 0.09555555555555556\n",
      "*Epoch [ 20 , 30 ], loss=  0.3220501557196904 , accuracy , 0.093\n",
      "*Epoch [ 21 , 30 ], loss=  0.3196663880315868 , accuracy , 0.09688888888888889\n",
      "*Epoch [ 22 , 30 ], loss=  0.31775141371258847 , accuracy , 0.099\n",
      "*Epoch [ 23 , 30 ], loss=  0.3168626827066247 , accuracy , 0.10155555555555555\n",
      "*Epoch [ 24 , 30 ], loss=  0.31598934934008865 , accuracy , 0.09344444444444444\n",
      "*Epoch [ 25 , 30 ], loss=  0.31368604299482994 , accuracy , 0.09866666666666667\n",
      "*Epoch [ 26 , 30 ], loss=  0.31230469009683776 , accuracy , 0.09544444444444444\n",
      "*Epoch [ 27 , 30 ], loss=  0.3127873355967717 , accuracy , 0.10144444444444445\n",
      "*Epoch [ 28 , 30 ], loss=  0.3107273150093129 , accuracy , 0.09566666666666666\n",
      "*Epoch [ 29 , 30 ], loss=  0.31006334722869927 , accuracy , 0.09755555555555556\n",
      "*Epoch [ 30 , 30 ], loss=  0.3088473479285474 , accuracy , 0.09566666666666666\n"
     ]
    }
   ],
   "source": [
    "fit(30, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2097b",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66eb87b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape=  torch.Size([1, 28, 28])\n",
      "label=  7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZVUlEQVR4nO3df2hV9/3H8dfV6m3qbi7LNLk3M2ahKCvGufljaubvLwazTWrTgm1hxH9cu6ogaSt1Ugz+YYqglOF0rAynTDf3h3VuippVEytpRhQ7rXMuapwpGjJTe29M9Yr18/0jeOk1afRc7/WdmzwfcMGcez7ed08PPj3emxOfc84JAAADg6wHAAAMXEQIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYecJ6gPvdvXtXV65cUSAQkM/nsx4HAOCRc04dHR3Kz8/XoEG9X+v0uQhduXJFBQUF1mMAAB5RS0uLRo4c2es+fe6f4wKBgPUIAIAUeJg/z9MWoc2bN6uoqEhPPvmkJk6cqA8//PCh1vFPcADQPzzMn+dpidCuXbu0YsUKrV69WidPntSMGTNUVlamy5cvp+PlAAAZypeOu2hPmTJFEyZM0JYtW+LbnnnmGS1cuFDV1dW9ro1GowoGg6keCQDwmEUiEWVnZ/e6T8qvhG7fvq0TJ06otLQ0YXtpaanq6+u77R+LxRSNRhMeAICBIeURunbtmr788kvl5eUlbM/Ly1Nra2u3/aurqxUMBuMPPhkHAANH2j6YcP8bUs65Ht+kWrVqlSKRSPzR0tKSrpEAAH1Myr9PaPjw4Ro8eHC3q562trZuV0eS5Pf75ff7Uz0GACADpPxKaOjQoZo4caJqamoSttfU1KikpCTVLwcAyGBpuWNCZWWlfvazn2nSpEmaNm2afvvb3+ry5ct69dVX0/FyAIAMlZYILVq0SO3t7Vq7dq2uXr2q4uJi7d+/X4WFhel4OQBAhkrL9wk9Cr5PCAD6B5PvEwIA4GERIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzKY9QVVWVfD5fwiMUCqX6ZQAA/cAT6fhNx44dq7///e/xrwcPHpyOlwEAZLi0ROiJJ57g6gcA8EBpeU+oqalJ+fn5Kioq0osvvqiLFy9+7b6xWEzRaDThAQAYGFIeoSlTpmj79u06ePCg3nvvPbW2tqqkpETt7e097l9dXa1gMBh/FBQUpHokAEAf5XPOuXS+QGdnp55++mmtXLlSlZWV3Z6PxWKKxWLxr6PRKCECgH4gEokoOzu7133S8p7QVw0bNkzjxo1TU1NTj8/7/X75/f50jwEA6IPS/n1CsVhMZ8+eVTgcTvdLAQAyTMoj9MYbb6iurk7Nzc36xz/+oRdeeEHRaFQVFRWpfikAQIZL+T/Hffrpp3rppZd07do1jRgxQlOnTlVDQ4MKCwtT/VIAgAyX9g8meBWNRhUMBq3HAAA8oof5YAL3jgMAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzKT9h9rh8XrhhRc8r1myZElSr3XlyhXPa27duuV5zY4dOzyvaW1t9bxGks6fP5/UOgDJ4UoIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZnzOOWc9xFdFo1EFg0HrMTLWxYsXPa/5zne+k/pBjHV0dCS17syZMymeBKn26aefel6zfv36pF7r+PHjSa1Dl0gkouzs7F734UoIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDzhPUASK0lS5Z4XvO9730vqdc6e/as5zXPPPOM5zUTJkzwvGb27Nme10jS1KlTPa9paWnxvKagoMDzmsfpzp07ntf873//87wmHA57XpOMy5cvJ7WOG5imH1dCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmDaz3zwwQePZU2yDhw48Fhe55vf/GZS677//e97XnPixAnPayZPnux5zeN069Ytz2v+85//eF6TzE1wc3JyPK+5cOGC5zV4PLgSAgCYIUIAADOeI3T06FEtWLBA+fn58vl82rNnT8LzzjlVVVUpPz9fWVlZmj17ts6cOZOqeQEA/YjnCHV2dmr8+PHatGlTj8+vX79eGzdu1KZNm9TY2KhQKKR58+apo6PjkYcFAPQvnj+YUFZWprKysh6fc87p3Xff1erVq1VeXi5J2rZtm/Ly8rRz50698sorjzYtAKBfSel7Qs3NzWptbVVpaWl8m9/v16xZs1RfX9/jmlgspmg0mvAAAAwMKY1Qa2urJCkvLy9he15eXvy5+1VXVysYDMYfBQUFqRwJANCHpeXTcT6fL+Fr51y3bfesWrVKkUgk/mhpaUnHSACAPiil36waCoUkdV0RhcPh+Pa2trZuV0f3+P1++f3+VI4BAMgQKb0SKioqUigUUk1NTXzb7du3VVdXp5KSklS+FACgH/B8JXTjxg2dP38+/nVzc7M+/vhj5eTkaNSoUVqxYoXWrVun0aNHa/To0Vq3bp2eeuopvfzyyykdHACQ+TxH6Pjx45ozZ07868rKSklSRUWFfv/732vlypW6efOmXnvtNV2/fl1TpkzRoUOHFAgEUjc1AKBf8DnnnPUQXxWNRhUMBq3HAODR888/73nNn//8Z89rPvnkE89rvvoXZy8+++yzpNahSyQSUXZ2dq/7cO84AIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmEnpT1YF0D/k5uZ6XrN582bPawYN8v734LVr13pew92w+y6uhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM9zAFEA3S5cu9bxmxIgRntdcv37d85pz5855XoO+iyshAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMNzAF+rEf/ehHSa176623UjxJzxYuXOh5zSeffJL6QWCGKyEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAw3MAX6sR//+MdJrRsyZIjnNR988IHnNR999JHnNehfuBICAJghQgAAM54jdPToUS1YsED5+fny+Xzas2dPwvOLFy+Wz+dLeEydOjVV8wIA+hHPEers7NT48eO1adOmr91n/vz5unr1avyxf//+RxoSANA/ef5gQllZmcrKynrdx+/3KxQKJT0UAGBgSMt7QrW1tcrNzdWYMWO0ZMkStbW1fe2+sVhM0Wg04QEAGBhSHqGysjLt2LFDhw8f1oYNG9TY2Ki5c+cqFov1uH91dbWCwWD8UVBQkOqRAAB9VMq/T2jRokXxXxcXF2vSpEkqLCzUvn37VF5e3m3/VatWqbKyMv51NBolRAAwQKT9m1XD4bAKCwvV1NTU4/N+v19+vz/dYwAA+qC0f59Qe3u7WlpaFA6H0/1SAIAM4/lK6MaNGzp//nz86+bmZn388cfKyclRTk6Oqqqq9PzzzyscDuvSpUv65S9/qeHDh+u5555L6eAAgMznOULHjx/XnDlz4l/fez+noqJCW7Zs0enTp7V9+3Z9/vnnCofDmjNnjnbt2qVAIJC6qQEA/YLPOeesh/iqaDSqYDBoPQbQ52RlZXlec+zYsaRea+zYsZ7XzJ071/Oa+vp6z2uQOSKRiLKzs3vdh3vHAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwEzaf7IqgNR48803Pa/5wQ9+kNRrHThwwPMa7oiNZHAlBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamgIGf/OQnnte8/fbbntdEo1HPayRp7dq1Sa0DvOJKCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwww1MgUf0rW99y/OaX/3qV57XDB482POa/fv3e14jSQ0NDUmtA7ziSggAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMNTIGvSOYmoQcOHPC8pqioyPOaCxcueF7z9ttve14DPE5cCQEAzBAhAIAZTxGqrq7W5MmTFQgElJubq4ULF+rcuXMJ+zjnVFVVpfz8fGVlZWn27Nk6c+ZMSocGAPQPniJUV1enpUuXqqGhQTU1Nbpz545KS0vV2dkZ32f9+vXauHGjNm3apMbGRoVCIc2bN08dHR0pHx4AkNk8fTDh/jdgt27dqtzcXJ04cUIzZ86Uc07vvvuuVq9erfLycknStm3blJeXp507d+qVV15J3eQAgIz3SO8JRSIRSVJOTo4kqbm5Wa2trSotLY3v4/f7NWvWLNXX1/f4e8RiMUWj0YQHAGBgSDpCzjlVVlZq+vTpKi4uliS1trZKkvLy8hL2zcvLiz93v+rqagWDwfijoKAg2ZEAABkm6QgtW7ZMp06d0h//+Mduz/l8voSvnXPdtt2zatUqRSKR+KOlpSXZkQAAGSapb1Zdvny59u7dq6NHj2rkyJHx7aFQSFLXFVE4HI5vb2tr63Z1dI/f75ff709mDABAhvN0JeSc07Jly7R7924dPny423d9FxUVKRQKqaamJr7t9u3bqqurU0lJSWomBgD0G56uhJYuXaqdO3fqL3/5iwKBQPx9nmAwqKysLPl8Pq1YsULr1q3T6NGjNXr0aK1bt05PPfWUXn755bT8BwAAMpenCG3ZskWSNHv27ITtW7du1eLFiyVJK1eu1M2bN/Xaa6/p+vXrmjJlig4dOqRAIJCSgQEA/YfPOeesh/iqaDSqYDBoPQYGqDFjxnhe8+9//zsNk3T37LPPel7z17/+NQ2TAA8nEokoOzu71324dxwAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMJPWTVYG+rrCwMKl1hw4dSvEkPXvzzTc9r/nb3/6WhkkAW1wJAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmuIEp+qWf//znSa0bNWpUiifpWV1dnec1zrk0TALY4koIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDDDUzR502fPt3zmuXLl6dhEgCpxpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5iiz5sxY4bnNd/4xjfSMEnPLly44HnNjRs30jAJkHm4EgIAmCFCAAAzniJUXV2tyZMnKxAIKDc3VwsXLtS5c+cS9lm8eLF8Pl/CY+rUqSkdGgDQP3iKUF1dnZYuXaqGhgbV1NTozp07Ki0tVWdnZ8J+8+fP19WrV+OP/fv3p3RoAED/4OmDCQcOHEj4euvWrcrNzdWJEyc0c+bM+Ha/369QKJSaCQEA/dYjvScUiUQkSTk5OQnba2trlZubqzFjxmjJkiVqa2v72t8jFospGo0mPAAAA0PSEXLOqbKyUtOnT1dxcXF8e1lZmXbs2KHDhw9rw4YNamxs1Ny5cxWLxXr8faqrqxUMBuOPgoKCZEcCAGSYpL9PaNmyZTp16pSOHTuWsH3RokXxXxcXF2vSpEkqLCzUvn37VF5e3u33WbVqlSorK+NfR6NRQgQAA0RSEVq+fLn27t2ro0ePauTIkb3uGw6HVVhYqKamph6f9/v98vv9yYwBAMhwniLknNPy5cv1/vvvq7a2VkVFRQ9c097erpaWFoXD4aSHBAD0T57eE1q6dKn+8Ic/aOfOnQoEAmptbVVra6tu3rwpqetWJG+88YY++ugjXbp0SbW1tVqwYIGGDx+u5557Li3/AQCAzOXpSmjLli2SpNmzZyds37p1qxYvXqzBgwfr9OnT2r59uz7//HOFw2HNmTNHu3btUiAQSNnQAID+wfM/x/UmKytLBw8efKSBAAADB3fRBr7in//8p+c1//d//+d5zWeffeZ5DdAfcQNTAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMCMzz3o1tiPWTQaVTAYtB4DAPCIIpGIsrOze92HKyEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABm+lyE+tit7AAASXqYP8/7XIQ6OjqsRwAApMDD/Hne5+6ifffuXV25ckWBQEA+ny/huWg0qoKCArW0tDzwzqz9GcehC8ehC8ehC8ehS184Ds45dXR0KD8/X4MG9X6t88RjmumhDRo0SCNHjux1n+zs7AF9kt3DcejCcejCcejCcehifRwe9kfy9Ll/jgMADBxECABgJqMi5Pf7tWbNGvn9futRTHEcunAcunAcunAcumTacehzH0wAAAwcGXUlBADoX4gQAMAMEQIAmCFCAAAzGRWhzZs3q6ioSE8++aQmTpyoDz/80Hqkx6qqqko+ny/hEQqFrMdKu6NHj2rBggXKz8+Xz+fTnj17Ep53zqmqqkr5+fnKysrS7NmzdebMGZth0+hBx2Hx4sXdzo+pU6faDJsm1dXVmjx5sgKBgHJzc7Vw4UKdO3cuYZ+BcD48zHHIlPMhYyK0a9curVixQqtXr9bJkyc1Y8YMlZWV6fLly9ajPVZjx47V1atX44/Tp09bj5R2nZ2dGj9+vDZt2tTj8+vXr9fGjRu1adMmNTY2KhQKad68ef3uPoQPOg6SNH/+/ITzY//+/Y9xwvSrq6vT0qVL1dDQoJqaGt25c0elpaXq7OyM7zMQzoeHOQ5ShpwPLkP88Ic/dK+++mrCtu9+97vurbfeMpro8VuzZo0bP3689RimJLn3338//vXdu3ddKBRy77zzTnzbrVu3XDAYdL/5zW8MJnw87j8OzjlXUVHhnn32WZN5rLS1tTlJrq6uzjk3cM+H+4+Dc5lzPmTEldDt27d14sQJlZaWJmwvLS1VfX290VQ2mpqalJ+fr6KiIr344ou6ePGi9Uimmpub1dramnBu+P1+zZo1a8CdG5JUW1ur3NxcjRkzRkuWLFFbW5v1SGkViUQkSTk5OZIG7vlw/3G4JxPOh4yI0LVr1/Tll18qLy8vYXteXp5aW1uNpnr8pkyZou3bt+vgwYN677331NraqpKSErW3t1uPZube//+Bfm5IUllZmXbs2KHDhw9rw4YNamxs1Ny5cxWLxaxHSwvnnCorKzV9+nQVFxdLGpjnQ0/HQcqc86HP3UW7N/f/aAfnXLdt/VlZWVn81+PGjdO0adP09NNPa9u2baqsrDSczN5APzckadGiRfFfFxcXa9KkSSosLNS+fftUXl5uOFl6LFu2TKdOndKxY8e6PTeQzoevOw6Zcj5kxJXQ8OHDNXjw4G5/k2lra+v2N56BZNiwYRo3bpyampqsRzFz79OBnBvdhcNhFRYW9svzY/ny5dq7d6+OHDmS8KNfBtr58HXHoSd99XzIiAgNHTpUEydOVE1NTcL2mpoalZSUGE1lLxaL6ezZswqHw9ajmCkqKlIoFEo4N27fvq26uroBfW5IUnt7u1paWvrV+eGc07Jly7R7924dPnxYRUVFCc8PlPPhQcehJ332fDD8UIQnf/rTn9yQIUPc7373O/evf/3LrVixwg0bNsxdunTJerTH5vXXX3e1tbXu4sWLrqGhwf30pz91gUCg3x+Djo4Od/LkSXfy5EknyW3cuNGdPHnS/fe//3XOOffOO++4YDDodu/e7U6fPu1eeuklFw6HXTQaNZ48tXo7Dh0dHe7111939fX1rrm52R05csRNmzbNffvb3+5Xx+EXv/iFCwaDrra21l29ejX++OKLL+L7DITz4UHHIZPOh4yJkHPO/frXv3aFhYVu6NChbsKECQkfRxwIFi1a5MLhsBsyZIjLz8935eXl7syZM9Zjpd2RI0ecpG6PiooK51zXx3LXrFnjQqGQ8/v9bubMme706dO2Q6dBb8fhiy++cKWlpW7EiBFuyJAhbtSoUa6iosJdvnzZeuyU6um/X5LbunVrfJ+BcD486Dhk0vnAj3IAAJjJiPeEAAD9ExECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABg5v8B02GnBBZO5SYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = MNIST(root = \"D:\\sourav\\cs\\AI_ML\\pytorch\\pytorch\", train = False, transform=trn.ToTensor())\n",
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap = 'gray')\n",
    "print('shape= ', img.shape)\n",
    "print('label= ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6088b948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be9be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    prob, pred = torch.max(yb, dim=1)\n",
    "    return pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88f02464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=  1 , prediction=  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[57]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('label= ', label, ', prediction= ', predict_img(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f881271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=  0.2911902453611256  accuracy=  0.0996\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=5)\n",
    "\n",
    "test_loss, total, test_acc = evaluation(model, loss_fn, test_loader, accuracy)\n",
    "print('Loss= ', test_loss, \" accuracy= \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797dc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
